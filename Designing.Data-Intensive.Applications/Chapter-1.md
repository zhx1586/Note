# 第一章 可靠、可扩展、可维护的应用

目前的应用大多是**数据密集型**的，而不是**计算密集型**的。CPU算力几乎不会成为这些应用的限制因素，这些应用的问题通常在于数据的数量、数据的复杂度以及数据的更新速度。

数据密集型应用通常是使用提供通用能力的标准模块搭建的。例如，大部分应用需要：

- 存储数据，以便当前应用或者其他应用能在之后查询到这些数据（数据库 databases）。

- 存储耗时操作的结果，来加速查询（缓存 caches）。

- 允许使用者通过关键词搜索数据，或者按照多种方式对数据进行过滤（搜索索引 search indexes）。

- 向其他应用发送消息，从而使任务被异步处理（流处理 stream processing）。

- 周期性地计算逐渐增加的数据的总量（批处理 batch processing）。

在需要搭建一个应用时，大多数工程师并不会选择从头开始编写一个数据存储引擎，因为数据库才是对于这个任务最合适的工具。

但是，实际情况往往也不会这么简单。目前还存着许多具有不同特性的数据系统，因为不同应用有着不同的要求。缓存和搜索索引的构建方式同样也是多种多样的。在构建一个应用时，我们仍然需要考虑，对于要处理的任务，最适合的方式是什么。并且，在单一工具无法满足应用的要求时，将多个工具组合使用也是具有挑战性的。

本书介绍了数据系统的设计准则和实例，以及如何使用这些数据系统来构建数据密集型应用。我们将会探讨不同工具之间的相同点和不同点，以及这些工具是如何实现这些特性的。

在本章中，我们将开始探讨可靠、可扩展、可维护应用的基本概念。我们将阐明这些该概念的含义，以及一些思考这些概念的方法，这将会是后续章节中所需要的。在后续章节中，我们将继续逐层介绍，构建数据密集型应用时需要考虑的设计决策。

## 对数据系统的理解

我们通常认为数据库、消息队列、缓存等是不同分类下的工具。尽管数据库和消息队列具有一些相似之处（都需要将数据存储一段时间），但是它们访问数据的方式是非常不同的，这意味着它们具有不同的特征，因此也有着不同的实现方式。

所以为什么我们将这些工具统一放在**数据系统**这个概念下呢？

近年来，出现了许多用于数据存储和数据处理的新工具。它们对多种不同的使用场景做了优化，因此不再能按照传统的方式进行分类。例如，Redis 是可以用作消息队列的数据存储系统，Kafka 是具有类似数据库的持久化能力的消息队列。分类之间的边界开始变得模糊来。

另一方面，越来越多的应用需要解决要求苛刻的或者适用性广的问题，以至于单个工具不再能满足其对数据存储和数据处理的需要。正相反，这些问题被拆分为了可以被单个工具高效处理的子任务，处理子任务的工具们组合在一起共同构成了应用代码。

例如，如果你需要一个应用管理的独立于主数据库之外的缓存层（使用 Memcached 等）或者全文搜索层（使用 Elasticsearch 等），那么通常你的应用代码应该负责保持缓存和索引与主数据库的同步性。

当你需要组合多个工具来搭建一个服务时，这个服务的接口通常会对客户端隐藏这些实现细节。此时，你有必要使用通用功能的小组件搭建一个全新的、特定功能的数据系统 。你搭建的数据系统应该提供某些方面的保证，比如：缓存在数据写入时得到了正确的校验或者更新，从而使客户端可以获得一致的结果。这就意味着你不仅是一位应用程序开发者，也是一位数据系统设计者。

如果你正在设计一个数据系统，需要考虑下面这些问题。如何保证数据是正确的和完整的，即使在系统发生故障时？如何向客户端持续提供可用的服务，即使在系统中的一部分被迫降级时？如何扩展系统来应对请求量的增长？如何设计优秀的系统对外接口？

有许多因素会影响数据系统的设计，包括设计者的技能和经验、遗留系统的依赖、交付时间的长短、对各种风险的承受能力、监管约束等。这些因素在不同场景下各不相同。

在本书中，我们重点关注对大多数软件系统都十分重要的三个因素：

- 可靠性：系统应该持续地正确工作（在预期的性能范围内提供正常的功能）即使在逆境中（硬件或者软件错误，甚至是人为因素导致的错误）。

- 可扩展性：在系统成长（数据量增加、请求量增加或者复杂度增加）过程中，应该存在合理的方式来应对这种变化。

- 可维护性：随着时间推移，会有许多不同的人参与到系统中（开发或者操作，包括维护现有功能和调整系统以适应新的用途），并且这些人可以高效地在系统之上工作。

这些名次经常被提及，但是却缺少对其含义的准确理解。出于严谨性的考虑，本章的剩余部分会探讨对可靠性、可扩展性和可维护性的理解。在后续的章节中，我们将会给出各种各样的用来实现这些特性的技术、架构和算法。

## 可靠性

对于一个软件，可靠性意味着：

- 应用可以提供使用者预期的功能。

- 可以容忍使用者犯错或者按照未预期的方式使用该软件。

- 在预期的请求量和数据规模下，可以良好工作。

- 系统会阻止未授权的访问或者破坏。

系统发现并处理错误的能力被称为**容错性**。上述对于可靠性的描述，可能会造成一种误解，即假设了我们可以构建一个可以容忍任何可能出现错误的系统，事实上这是不可行的。因此，只讨论特定种类的错误才是有意义的。

应注意**错误**和**失败**是不同的。**错误**通常是指系统中的部分组件功能异常，而**失败**是指系统整体停止对外提供服务。将错误出现的可能性降为零是不可能，因此最好是去设计一些容错机制来避免**错误**进一步引起**失败**。在本书中，我们将介绍多种机制来基于**不可靠**的组件构建**可靠**的系统。

与直觉相反的是，在这种具有容错性的系统中，通过主动触发的方式来提高错误产生的频率，是有意义的。许多严重的问题实际上都是由于缺少错误处理机制导致的，通过故意引发错误，可以确保容错机制被持续的测试和验证，进一步可以提升对于自然产生的错误可以被妥善处理的信心。

尽管我们通常倾向于容忍错误而不是避免错误，还是存在一些避免错误优于处理错误的场景，比如当某些错误实际上无法被任何手段妥善处理时。然而，本书仍旧重点关注可以被妥善处理的错误，即下面几个部分将要介绍的内容。

## 硬件错误

在讨论系统失败的原因时，硬件错误往往是最先想到的。硬盘故障、内存故障、停电或者拔网线等。在拥有海量机器的大型数据中心，这些问题无时无刻不在出现。

据统计硬盘的**故障平均时间（mean time to failure MTTF）**为10-50年。因此，在一个包含10000张硬盘的存储集群中，平均每天就有可能有一张硬盘发生故障。

我们首先想到的应对方法是对单个硬件组件增加冗余，从而降低系统的失败率。硬盘可以组成RAID、服务器可以使用双路供电和热插拔CPU、数据中心可以使用电池或者柴油发电机作为备份电源。当一个组件故障时，备份组件可以作为替换。这种方式不能完全避免硬件问题，但是容易理解并且通常可以让机器数年不中断工作。

直到最近，硬件冗余对于大多数系统来说都是有效，因为这种方式可以使单个硬件的完全失败变得几乎不可能。只要可以做到快速替换，就不会导致灾难性的事故。因此，多机冗余只有少数要求完全确保高可用的应用才会需要。

然而，对于数据容量和计算能力的需求在不断增长，更多的应用开始使用更多的机器，这也同比例地增加了硬件错误的出现频率。因此，出现了一些通过软件容错机制而不是硬件冗余方式来避免硬件错误的系统。这些系统具有操作性上的优势：单机系统需要一些规划预留的停止服务时间，如果需要重启机器（比如：安装操作系统安全补丁），此时具有软件容错机制的系统可以每次处理一个节点，而不需要整个系统的停止服务时间。

## 软件错误

