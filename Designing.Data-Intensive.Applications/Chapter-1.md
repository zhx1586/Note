# 第一章 可靠、可扩展、可维护的应用

目前的应用大多是**数据密集型**的，而不是**计算密集型**的。CPU算力几乎不会成为这些应用的限制因素，这些应用的问题通常在于数据的数量、数据的复杂度以及数据的更新速度。

数据密集型应用通常是使用提供通用能力的标准模块搭建的。例如，大部分应用需要：

- 存储数据，以便当前应用或者其他应用能在之后查询到这些数据（数据库 databases）。

- 存储耗时操作的结果，来加速查询（缓存 caches）。

- 允许使用者通过关键词搜索数据，或者按照多种方式对数据进行过滤（搜索索引 search indexes）。

- 向其他应用发送消息，从而使任务被异步处理（流处理 stream processing）。

- 周期性地计算逐渐增加的数据的总量（批处理 batch processing）。

在需要搭建一个应用时，大多数工程师并不会选择从头开始编写一个数据存储引擎，因为数据库才是对于这个任务最合适的工具。

但是，实际情况往往也不会这么简单。目前还存着许多具有不同特性的数据系统，因为不同应用有着不同的要求。缓存和搜索索引的构建方式同样也是多种多样的。在构建一个应用时，我们仍然需要考虑，对于要处理的任务，最适合的方式是什么。并且，在单一工具无法满足应用的要求时，将多个工具组合使用也是具有挑战性的。

本书介绍了数据系统的设计准则和实例，以及如何使用这些数据系统来构建数据密集型应用。我们将会探讨不同工具之间的相同点和不同点，以及这些工具是如何实现这些特性的。

在本章中，我们将开始探讨可靠、可扩展、可维护应用的基本概念。我们将阐明这些该概念的含义，以及一些思考这些概念的方法，这将会是后续章节中所需要的。在后续章节中，我们将继续逐层介绍，构建数据密集型应用时需要考虑的设计决策。

## 对数据系统的理解

我们通常认为数据库、消息队列、缓存等是不同分类下的工具。尽管数据库和消息队列具有一些相似之处（都需要将数据存储一段时间），但是它们访问数据的方式是非常不同的，这意味着它们具有不同的特征，因此也有着不同的实现方式。

所以为什么我们将这些工具统一放在**数据系统**这个概念下呢？

近年来，出现了许多用于数据存储和数据处理的新工具。它们对多种不同的使用场景做了优化，因此不再能按照传统的方式进行分类。例如，Redis 是可以用作消息队列的数据存储系统，Kafka 是具有类似数据库的持久化能力的消息队列。分类之间的边界开始变得模糊来。

另一方面，越来越多的应用需要解决要求苛刻的或者适用性广的问题，以至于单个工具不再能满足其对数据存储和数据处理的需要。正相反，这些问题被拆分为了可以被单个工具高效处理的子任务，处理子任务的工具们组合在一起共同构成了应用代码。

例如，如果你需要一个应用管理的独立于主数据库之外的缓存层（使用 Memcached 等）或者全文搜索层（使用 Elasticsearch 等），那么通常你的应用代码应该负责保持缓存和索引与主数据库的同步性。

当你需要组合多个工具来搭建一个服务时，这个服务的接口通常会对客户端隐藏这些实现细节。此时，你有必要使用通用功能的小组件搭建一个全新的、特定功能的数据系统 。你搭建的数据系统应该提供某些方面的保证，比如：缓存在数据写入时得到了正确的校验或者更新，从而使客户端可以获得一致的结果。这就意味着你不仅是一位应用程序开发者，也是一位数据系统设计者。

如果你正在设计一个数据系统，需要考虑下面这些问题。如何保证数据是正确的和完整的，即使在系统发生故障时？如何向客户端持续提供可用的服务，即使在系统中的一部分被迫降级时？如何扩展系统来应对请求量的增长？如何设计优秀的系统对外接口？

有许多因素会影响数据系统的设计，包括设计者的技能和经验、遗留系统的依赖、交付时间的长短、对各种风险的承受能力、监管约束等。这些因素在不同场景下各不相同。

在本书中，我们重点关注对大多数软件系统都十分重要的三个因素：

- 可靠性：系统应该持续地正确工作（在预期的性能范围内提供正常的功能）即使在逆境中（硬件或者软件错误，甚至是人为因素导致的错误）。

- 可扩展性：在系统成长（数据量增加、请求量增加或者复杂度增加）过程中，应该存在合理的方式来应对这种变化。

- 可维护性：随着时间推移，会有许多不同的人参与到系统中（开发或者操作，包括维护现有功能和调整系统以适应新的用途），并且这些人可以高效地在系统之上工作。

这些名次经常被提及，但是却缺少对其含义的准确理解。出于严谨性的考虑，本章的剩余部分会探讨对可靠性、可扩展性和可维护性的理解。在后续的章节中，我们将会给出各种各样的用来实现这些特性的技术、架构和算法。

## 可靠性

对于一个软件，可靠性意味着：

- 应用可以提供使用者预期的功能。

- 可以容忍使用者犯错或者按照未预期的方式使用该软件。

- 在预期的请求量和数据规模下，可以良好工作。

- 系统会阻止未授权的访问或者破坏。

系统发现并处理错误的能力被称为**容错性**。上述对于可靠性的描述，可能会造成一种误解，即假设了我们可以构建一个可以容忍任何可能出现错误的系统，事实上这是不可行的。因此，只讨论特定种类的错误才是有意义的。

应注意**错误**和**失败**是不同的。**错误**通常是指系统中的部分组件功能异常，而**失败**是指系统整体停止对外提供服务。将错误出现的可能性降为零是不可能，因此最好是去设计一些容错机制来避免**错误**进一步引起**失败**。在本书中，我们将介绍多种机制来基于**不可靠**的组件构建**可靠**的系统。

与直觉相反的是，在这种具有容错性的系统中，通过主动触发的方式来提高错误产生的频率，是有意义的。许多严重的问题实际上都是由于缺少错误处理机制导致的，通过故意引发错误，可以确保容错机制被持续的测试和验证，进一步可以提升对于自然产生的错误可以被妥善处理的信心。

尽管我们通常倾向于容忍错误而不是避免错误，还是存在一些避免错误优于处理错误的场景，比如当某些错误实际上无法被任何手段妥善处理时。然而，本书仍旧重点关注可以被妥善处理的错误，即下面几个部分将要介绍的内容。

### 硬件错误

在讨论系统失败的原因时，硬件错误往往是最先想到的。硬盘故障、内存故障、停电或者拔网线等。在拥有海量机器的大型数据中心，这些问题无时无刻不在出现。

据统计硬盘的**故障平均时间**（mean time to failure MTTF）为10-50年。因此，在一个包含10000张硬盘的存储集群中，平均每天就有可能有一张硬盘发生故障。

我们首先想到的应对方法是对单个硬件组件增加冗余，从而降低系统的失败率。硬盘可以组成RAID、服务器可以使用双路供电和热插拔CPU、数据中心可以使用电池或者柴油发电机作为备份电源。当一个组件故障时，备份组件可以作为替换。这种方式不能完全避免硬件问题，但是容易理解并且通常可以让机器数年不中断工作。

直到最近，硬件冗余对于大多数系统来说都是有效，因为这种方式可以使单个硬件的完全失败变得几乎不可能。只要可以做到快速替换，就不会导致灾难性的事故。因此，多机冗余只有少数要求完全确保高可用的应用才会需要。

然而，对于数据容量和计算能力的需求在不断增长，更多的应用开始使用更多的机器，这也同比例地增加了硬件错误的出现频率。因此，出现了一些通过软件容错机制而不是硬件冗余方式来避免硬件错误的系统。这些系统具有操作性上的优势：单机系统需要一些规划预留的停止服务时间，如果需要重启机器（比如：安装操作系统安全补丁），此时具有软件容错机制的系统可以每次处理一个节点，而不需要整个系统的停止服务时间。

### 软件错误

我们通常认为硬件错误是随机且独立的，比如一台机器的硬盘故障并不意味着另外一台的机器的硬盘会发生故障。硬件错误之间可能是具有微弱关联的（由于共同的原因导致的硬件错误，例如服务器阵列的温度过高），但是大量硬件同时发生故障也是不太可能出现的。

另一种类型的错误是系统内部错误。这种错误是更加难预料的，因为这种错误在节点之间具有关联性，与关联性较弱的硬件错误相比，这种错误更容易导致系统失败。比如：

- 会在接收特定异常输入时导致应用服务崩溃的软件BUG。

- 泄漏的进程耗尽了所有的共享资源，比如CPU时间、内存空间、硬盘空间护着网络带宽。

- 系统的依赖服务响应变慢，甚至停止响应。

- 级联失败，一个组件的错误会导致另外一个组件的错误，进一步导致更大范围的错误。

会导致这些软件错误的BUG往往会在系统中潜藏很长时间，直到被某种特定的场景触发。这种场景往往是，软件对于其运行环境做了某种假设，这种假设在通常情况下是满足的，由于某种原因突然变得不满足。

对于软件的系统性错误问题，没有快速的解决方法。但是一些措施可能会有有助于避免这类问题：确认系统中的假设和相互关联是否合理；测试验证；进程隔离；允许进程崩溃和重启；对生产环境的系统指标进行度量、监控和分析。如果系统需要提供某种保证（比如：在消息队列中，入队列的消息数量与出队列的消息数量相等），应该在运行时进行持续的自我检查，并且在出现差异时告警。

### 人为错误

软件系统是由人来设计和构建的，维持系统运行的操作者也是人。即使在专心致志的情况下，人也是不可靠的。

尽管人是不可靠的，我们如何来构建可靠的系统呢？

- 在设计系统时尽可能地减少出错的机会。比如，良好设计的API和管理接口可以减少很多错误。然而，接口也不能要求过于严格而限制了人的主观能动性，二者之间应该进行权衡。

- 将人犯错最多的地方与错误能够导致失败的地方解耦。比如，提供完全模拟生产环境的沙箱环境，在沙箱环境中人可以按照真实的想法实验和探索，而不会影响真实的用户。

- 完整的分层测试，单元测试、集成测试和人工测试。自动化测试是广泛使用、容易理解的，在覆盖边界场景时尤其有用。

- 允许快速简单地从人为错误中恢复，来降低这种失败可能造成的影响。比如：提升配置变更的回滚速度、灰度发布、提供数据修复工具。

- 搭建详细、清晰的监控，例如性能指标和错误率监控。监控可以提供更早的告警信号，并且可以检查对系统的假设和约束是否被打破。

- 做好训练和实践的管理。

### 可靠性的重要程度

可靠性不仅仅是核电站或者航空控制系统软件的要求，日常应用的软件也需要可靠性。商业软件的BUG可能会造成财产损失，电商网站的宕机可能会造成收入损失和口碑损失。

即使在非关键性的应用中，我们也应该为用户负责。比如当一对夫妻将他们的照片和孩子的视频存放在你的相册应用中时。当数据库突然崩溃时，他们会作何感想？他们是否知道如何从备份中恢复数据？

有许多场景是我们应该严格保证可靠性来降低开发损失（比如，开发一个新市场的原型产品时）或者运营损失（对于利润微薄的服务），但是，我们也应该敏锐地察觉到什么时候我们应该放宽对可靠性的要求。

## 可扩展性

即使系统目前能够可靠地运行时，这并不意味着系统一定可以在未来继续可靠地运行。一个常见的原因是负载的增加，系统的并发访问用户数量可能从1万增加到10万，或者从一百万增加到一千万。在这个过程中，系统要处理的数据量可能也会大量增加。

可扩展性是指系统应对负载增加的能力。

### 描述负载

首先，我们需要简洁地描述系统的当前负载。用来描述负载的参数的最佳选择，取决于系统的架构：对于Web服务器来说，应该使用每秒的请求量；对于数据库来说，应该使用读请求和写请求的比例；对于聊天室来说，应该使用同一时刻的活跃用户数量；对于缓存来说，应该使用命中率。有时候平均值是具有参考意义的，也有可能少数极端场景才是系统的瓶颈。

## 描述性能

当我们已经可以描述系统中的负载时，我们就可以研究在负载增加时发生了什么。可以按照下面两个思路进行：

- 当负载增加，系统资源保持不变时，系统性能会受到什么影响？

- 当负载增加，为了维持系统性能不变，需要增加多少系统资源？

回答这两个问题都需要系统性能的量化指标，因此我们简要地看一下如何描述系统的性能。

对于批处理系统（比如 Hadoop），我们通常使用**吞吐量**作为系统的性能指标，吞吐量是指系统每秒钟能够处理的任务数量，或者在固定大小的数据集上执行任务花费的总时间。对于在线系统，我们通常使用**响应时间**作为系统的性能指标，响应时间是指客户端发送请求到收到响应之间的时间。

**延迟**和**响应时间**经常被用作同义词，但是它们的含义并不相同。响应时间是客户端视角下的，除了服务端处理请求的时间，还包括网络延迟和队列等待。延迟是一个请求等待被服务端处理的时间。

即使一个请求被重复地发送，每次的响应时间之间都会有细微的差异。实际上，在系统处理多种不同的请求时，响应时间之间的差异是巨大的。因此，响应时间不是一个单独的数值，而是一个测量值的分布。

通常会使用服务端上报的平均响应时间作为系统的性能指标。然而，在需要知道响应的"典型"时间时，平均值就不是一个合适的指标了，因为平均值无法描述有多少用户实际体验到了这些时延。通常更好的方式是使用百分位数（比如中位数 p50）。

为了度量最坏情况的响应时间，可以使用更大的百分位数，比如 p95，p99 或者 p999，这些指标很重要，因为其直接影响了用户对于服务的体验。优化高百分位的响应时间往往是困难的，因为这些指标很容易受到一些随机因素的影响。

队列延迟通常是导致大部分高百分位响应时间请求的原因。由于服务器只能并行处理少量的请求，只需要少数的慢请求就能够阻塞大部分后续请求，这种现象有时被称为**队头阻塞（head-of-line blocking）**效应。即使这些后续请求实际上可以被服务器快速处理，由于需要等待前面的慢请求处理完成，客户端看到的是很慢的总响应时间。出于这种考虑，从客户端来度量响应时间是重要的。

在人为地增加负载来测试系统地可扩展性时，客户端需要不间断地发送请求。如果客户端等待前一个请求执行完成后再发送下一个请求，测试时等待队列地长度会比实际生产环境地更短，导致测试结果失真。

### 应对负载的方法

在讨论了负载的描述方式和性能的计算指标之后，我们可以开始讨论最开始提到的可扩展性问题：即使在负载增加时，如何保持良好的性能？

只适用于一种等级的负载的架构，不太可能处理10倍负载的场景。如果你正在开发快速增长的服务，你需要经常反思在不同数量级的负载增加时，系统架构能否应对。

系统扩展的方式主要有两种：**垂直扩展**，即使用性能更强大的机器；**水平扩展**，即将负载分布到多个性能不那么强大的机器上。将负载分布到多个机器上也被称为**无共享（shared-nothing）**架构。能够运行在单机上的系统通常是更简单的，由于高性能的机器价格也更昂贵，需要应对复杂工作的系统往往不能避免水平扩展架构。实际上，良好的架构设计通常会务实地将两种方式结合起来使用：比如，使用多个性能相当强大的机器仍然比使用多个性能不够好的虚拟机更简单和经济。

许多系统是具有**可伸缩性**的，即这些系统可以自动在负载增加时，增加计算资源，而不是依靠手动地调整（依靠人来对系统的容量做出分析，然后增加更多机器）。具有可伸缩性的系统在负载具有高度不可预测性时是十分有用的，手动调整系统更加简单，造成意外操作的可能性也更小。

尽管将一个**无状态**的服务分布到多个机器上是相当简单直接的，将**有状态的**数据系统从单机迁移到分布式集群上可能会导致很多额外的复杂性。出于这种考虑，直到最近，保持数据库部署在单一节点上（垂直扩展）仍旧是推荐的选择，除非是由于升级的开销或者高可用性要求而不得不选择分布式部署。

随着分布式系统的工具和理论变得越来越完善，推荐的选择可能也会发生改变，至少在部分特定种类的系统上。可以预测到的是，在将来分布式数据系统会逐渐变成默认选择，即使在数据量和请求量不那么大的使用场景上。在本书的后续章节中，我们将会涉及多种分布式数据系统，这种系统架构不仅可以提供可扩展性，还更容易使用和提供可维护性。

用于处理海量业务的系统架构通常是为特定的应用场景定制的，并不存在通用的可扩展性架构。系统的瓶颈可能是读请求量、写请求量、存储的数据数量、数据的复杂性、响应时间要求、数据访问模式或者是上面几个问题的结合。

有良好扩展能力的架构，是基于对具体应用的一些假设条件构建的，这些条件包括什么样的操作是常见的、什么样的操作是罕见的。当这些假设不再满足的时候，系统的扩展能力就会变差，甚至可能会适得其反。在系统搭建的初期或者开发未经验证的产品时，通常更重要的点在于产品功能的快速迭代，而不是应对未来可能出现的负载的扩展能力。

即使对于具体的应用，应对负载的方法是不同的，可扩展的架构仍旧是基于通用功能组件、按照熟悉的模式搭建的。在本书中我们会介绍这些组件和模式。

## 可维护性

众所周知的是，软件的主要成本不在于最初版本的开发，而在于持续的维护，包括BUG修复、维持系统正常运行、排查错误、将其适配到新平台、将其修改以应对新的场景、偿还技术债、增加新特性。

不幸的是，软件开发人员并不喜欢维护被称为"遗产"的系统，可能的原因包括：修复前人犯下的错误；在过时的平台上开发；系统被迫去实现之前从未规划过的功能。每一个历史遗留系统都有各自让人不快的方式，因此也很难给出一个处理这类系统的通用建议。

然而，我们可以而且应该按照最小化维护成本的方式来设计软件，从自身做起避免历史遗留系统的出现。为了达到这个目的，我们将专注于下面三条软件系统的设计准则：

- 可操作性：使操作团队可以轻松地维持系统的平稳运行。

- 简单性：使新来的开发人员可以轻松地理解系统，尽可能多地减少系统的复杂性。（注意这不等同与用户界面的简单性。）

- 演化能力：使开发人员可以轻松地变更系统，在系统的要求发生变更时能够适配未预期的使用场景。有时也称为可扩展性、可变更性或者可塑性。

### 可操作性

尽管一些操作可以而且应该自动化执行，但是仍然需要人来启动并且确保工作正常。

操作团队对于维持软件系统的平稳运行是很重要的。一个好的操作团队通常应该负责处理：

- 监控系统的健康状态，在系统状态不佳时快速恢复服务。

- 定位问题的原因，比如是系统失败还是性能降级。

- 保持软件和平台是最新的，包括安全补丁。

- 维护不同系统之间相互影响关系的表格，从而及时避免有问题的操作。

- 预估未来可能出现的问题，并且将其在出现前解决。

- 构建良好的操作准则和工具，对于应用部署、配置管理等场景。

- 处理复杂的维护工作，例如将应用从一个平台迁移到另一个平台。

- 定义使操作可预测的流程，这有助于维持线上系统的平稳运行。

- 保留系统相关的组织信息，比如单个人员的去留。

良好的可操作性意味着使例行任务简单化，从而使操作团队能够专注于更有价值的事。数据系统可以通过很多措施来做到例行任务简单化，包括：

- 提供系统运行状态和内部信息的可视化界面，以及良好的监控系统。

- 提供对自动化和标准工具集成的良好支持。

- 避免对单个机器的依赖（允许机器停机维护，同时保证系统整体的不间断运行）。

- 提供良好的文档和容易理解的操作模型。

- 提供良好的默认行为，并且允许管理员在需要时可以自由地覆盖默认行为。

- 提供合适的自我恢复能力，并且允许管理员在需要时可以手动控制系统状态。

- 罗列出可预测的行为，尽量减少意外情况。

### 简单性

### 演进能力

