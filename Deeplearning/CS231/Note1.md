# CS231n Lecture Notes 

## 卷积层

### 概述 

卷积层的参数包含许多可学习的滤波器。每个滤波器在空间尺寸上小于输入量，但是深度与输入量相同。

### 局部连接性

局部连接是指，将神经元仅仅与输入量的部分区域相连接。在空间维度上采用局部连接，在深度上采用全连接。

### 空间分布

使用三个超参数来控制输出量的大小。

- 深度：输出量的深度对应着所使用的滤波器的个数。
- 步长：进行卷积运算时每次在空间上移动的像素个数。
- 零填充：对输入量的边缘用零进行填充。

$$ N = \frac{W-F+2P}{S} + 1 $$

其中，N：输出量、W：输入量、F：卷积核、P：零填充、S：步长。

### 参数共享

对输入量在深度这个维度上进行切片，处理每个切片时使用相同的权重和偏置。

### 总结

- 接收大小为 $W_1 \times H_1 \times D_1$ 的输入量
- 确定四个超参数：
    - 滤波器个数 $K$
    - 滤波器大小 $F$
    - 卷积步长 $S$
    - 零填充数 $P$
- 产生大小为 $W_2 \times H_2 \times D_2$ 的输出量，其中：
    - $W_2 = (W_1-F+2P)/S + 1$ 
    - $H_2 = (H_2-F+2P)/S + 1$
    - $D2 = K$
- 通过参数共享，参数总数降低为 $(F \cdot F \cdot D_1) \cdot K$ 个权重和 $K$ 个偏置
- 在输出量中，第 $d$ 个切片（大小为 $W_2 \times H_2$）是对输入量和第 $d$ 个滤波器进行步长为 $K$ 的卷积、并与第 $d$ 个偏置作用后的结果。

## 池化层

### 概述

在卷积神经网络的架构中通常会定期在连续的卷积层之间插入池化层。
这样做是为了更好地降低网络中的参数总量和计算量，同时也可以控制过拟合。
卷积层对输入在深度这一维度上的每个切片进行独立的操作，通过取最大值的操作来改变空间尺寸。
通常形式为使用一个大小为 $2\times2$ 的滤波器以 $2$ 为步长对输入量每个深度切片进行空间维度上的子采样，丢弃 $75%$ 的激活值。
每个最大值操作在这种情况下会从每个深度切片的 $2\times2$ 邻域内寻找一个最大值。
而深度维度保持不变。

### 总结

- 接收大小为 $W_1 \times H_1 \times D_1$ 的输入量
- 确定两个超参数：
    - 空间延展 $F$
    - 步长 $S$
- 产生大小为 $W_2 \times H_2 \times D_2$ 的输出量，其中：  
    - $W_2 = (W_1-F)/S + 1$ 
    - $H_2 = (H_2-F)/S + 1$
    - $D2 = K$
- 由于对输入进行一个固定的操作，池化层不会引入参数
- 池化层中通常不使用零填充

常用的池化参数为 $(F,S) = (3,2)$ 或 $(F,S)=(2,2)$，池化范围过大会带来负面影响。

### 广义池化

除了最大值池化，池化单元也可以采用其他形式，例如平均值池化或者 $L2-norm$ 池化。

## 案例研究

 - LeNet：卷及神经网络的第一个成功应用。识别邮政编码、数字。
 - AlexNet：使卷积神经网络在计算机视觉领域流行起来第一个成功案例。
 - ZFNet：ILSVRC2013年冠军。在AlexNet的基础上改进而来，减少第一个卷积层的滤波器尺寸和步长，增大中间的卷积层的尺寸。
 - GoogLeNet：ILSVRC2014年冠军。它的主要贡献在于对于开始模块的发展，这极大地降低了网络中的参数数量。最近的版本为 Inception-V4。
 - VGGNet：ILSVRC2014年亚军。它的主要贡献在于证明了网络深度是影响最终表现的关键因素。
 - ResNet：ILSVRC2015年冠军。它的特点在于独特的“跳过连接”和对批量规范化的大量使用。它的架构中也没有使用全连接层作为网络的结束。ResNet是目前最先进的卷积神经网络模型，也是实践中使用卷积神经网络的默认选择（2016.5.10）。