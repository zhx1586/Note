# YOLO_V2

## 摘要

- 我们引入了 YOLO9000，一个最先进的实时目标检测系统，该系统可以检测超过 $9000$ 个目标种类。
- 首先，我们提出了对 YOLO 检测算法的多种改进，既有新颖的思路也有之前工作中得到的启发。改进后的模型，YOLOv2 ，在标准的检测任务，PASCAL VOC 和 COCO，上是最先进的。通过使用一种新颖的、多尺度的训练方法，同一个 YOLOv2 模型可以在不同的尺寸下运行，提供了性能与精度的简单折中。在检测速度为 $67 FPS$ 时，YOLOv2 在VOC2007 数据集上达到了 $76.8 \ {\rm mAP}$ 的检测精度。在检测速度为 $40 FPS$ 时，YOLOv2 达到了 $78.6 \ {\rm mAP}$ 的检测精度，精度超过了最先进的 Faster RCNN 和 SSD ，而且运行地明显更快。
- 最后， 我们提出了一种将目标检测和分类的训练结合在一起的方法。使用这种方法，我们在 COCO 检测数据集和 ImageNet 分类数据集上同时训练了 YOLO9000。我们的联合训练使得 YOLO9000 可以对没有标签检测数据的类进行检测。我们在 ImageNet 检测任务上验证了我们的方法。YOLO9000 在 ImageNet 检测验证集上达到了 $19.7 \ {\rm mAP}$ 的检测精度，尽管训练时只有 $200$ 个待测分类中 $44$ 个分类的检测数据。在 $156$ 个不在 COCO 数据集中的分类上，YOLO9000 达到了 $16.0 \ {\rm mAP}$ 的检测精度。但是 YOLO9000 可以检测多于 $200$ 个分类，可以实现对超过 $9000$ 个不同目标分类的检测。并且该算法仍可实时运行。

## 1 引言

- 通常目的的目标检测应该是快速、准确、能够识别种类繁多的目标的。神经网络被引入后，检测框架变得逐渐快速、准确。然而，大多数检测方法能够检测的目标数量仍然很有限。当前的目标检测数据集与其他任务的数据集（例如：分类、标签）相比是十分有限的。常见的检测数据集大多包含几千到几十万张图片以及几十上百个标签。而分类数据集含有上百万张图片以及几万到几十万个分类。我们希望检测能够扩展到对象分类的级别。然而，为检测标记图片远比分类和标签等昂贵得多。因此我们不可能在短期内看到与分类数据集规模相当的检测数据集。
- 我们提出了一种新方法来利用我们已有的大量分类数据，并用它来扩展当前检测系统的范围。我们的方法使用目标检测的分层视角，允许我们将不同的数据集结合起来。我们也提出了一种既能在检测数据又能在分类数据上训练目标检测器的联合训练算法。我们的方法利用标记的检测图像来学习精确定位对象，同时使用分类图像来增加其词汇量和鲁棒性。
- 我们使用这种方法训练了 YOLO9000 ，一个可以识别超过 $9000$ 种不同目标分类的实时目标检测器。首先，我们在 YOLO 的基础上改进，得到了 YOLOv2，一个先进的实时检测器。之后我们使用数据集结合方法和联合训练算法在包含超过 $9000$ 个类别的 ImageNet 数据集和目标检测 COCO 数据集上训练了模型 YOLO9000。

## 2 更好（Better）

- YOLO 与当前最先进的方法相比还有许多缺点。YOLO 与 Faster R-CNN 的误差对比分析显示 YOLO 的定位误差明显更多。而且，YOLO 与基于区域推荐的方法相比召回（recall，敏感性）相对更低。因此，我们主要专注于在保持分类精度的基础上提高召回和定位精度。

- 计算机视觉通常趋向于更大，更深的网络。更好的性能通常取决于训练更大的网络或者将多个模型集合在一起。然而，对于 YOLOv2 我们的期待是更准确但仍然快速的检测器。我们简化网络结构并且使表达更加易于学习，而不是增大网络的规模。我们汇集过去工作                                        中的各种想法，与我们自身新颖的概念相结合，来提高 YOLO 的性能。

- **批量标准化（Batch Normalization）。** 批量标准化对于收敛性有明显的提高，同时不需要其他形式的规范化。通过向 YOLO 中的全部卷积层增加批量标准化，我们获得了 ${\rm mAP}$ 上 $2\%$ 的提高。批量标准化也有助于规范模型。使用批量标准化后我们可以去掉 Dropout 层而不产生过拟合。

- **高分辨率分类器（High Resolution Classifier）。** 
  - 所有最先进的检测方法都使用在 ImageNet 上预训练得到的分类器。从 AlexNet 开始大多数分类器处理的输入图片大小都不超过 $256 \times 256$ 。最开始的 YOLO 使用 $224 \times 224$ 的输入图片大小训练分类网络，之后将分辨率提升到 $448 \times 448$ 用于检测。这意味着网络不得不在转向学习目标检测的同时调整到新的输入分辨率。
  - 对于 YOLOv2 我们首先将分类网络在 ImageNet 数据集上进行 $10$ 个轮次的 $448 \times 448$ 全分辨率的微调。这给了网络足够的时间来调整自身具有的滤波器从而更好地工作在高分辨率输入下。之后，我们再对得到的网络进行检测任务的微调。高分辨率分类网络给我们带来了接近 $4\%$ 的 ${\rm mAP}$ 上的提升。

- **带有锚框的卷积（Convolutional With Anchor Boxes）。** 
  - YOLO 使用卷积特征提取器顶部的全连接层来预测边界框的坐标。Faster R-CNN 使用人工选择的先验信息来预测边界框而不是直接预测坐标。Faster R-CNN 中的区域推荐网络（RPN）仅使用卷积层来预测锚框的偏移量和置信度。由于预测层是卷积层，RPN在特征映射上的每个位置都预测这些偏移量。预测偏移量而不是坐标简化了问题，同时也使网络学习起来更加容易。
  - 基于此，我们去掉了 YOLO 中的全连接层，转为使用锚框来预测边界框。首先，我们去掉了一个池化层来使网络的卷积层的输出分辨率更高。我们也对网络进行了压缩，将输入图片的尺寸由 $448 \times 448 $ 减少到 $416 \times 416$ 。我们这样做是为了使特征映射上的位置数量为奇数，从而只有一个中心单元。目标，尤其是大目标，趋向于占据图像的中心，因此最好是存在一个位于正中心的位置来预测这些目标，而不是 $4$ 个相互紧邻着的位置。YOLO 的卷积层使用一个值为 $32$ 的因子来对图片进行降采样，因此使用分辨率为 $416 \times 416$ 的输入图片时，我们可以获得一个 $13 \times 13$ 的特征映射。
  - 在转向使用锚框后，我们放弃了基于空间位置的分类预测机制，而是为每个锚框预测目标和类别。延续 YOLO 的思路，对目标的预测仍然是推荐框与真实框之间的 ${\rm IOU}$ ，对分类的预测仍然是给定目标存在条件下条件概率。
  - 使用锚框后，我们在精度上有略微的降低。YOLO 在每张图片上仅预测 $98$ 个框，但是使用锚框后我们的模型可以预测超过 $1000$ 个框。不是用锚框的半成品模型精度为 $69.5 {\rm mAP}$ ，召回为 $81 \%$ 。使用锚框后精度为 $69.2 {\rm mAP}$ ，召回为 $88 \%$ 。 尽管精度上有降低，召回上的增加意味着我们的模型有了更多的提升空间。

- **维度聚类（Dimension Clusters）。**  

  - 在将锚框应用于 YOLO 时我们遇到了两个问题。第一个问题是框的维度是手工选择的。网络可以学习适当地调整框参数，但是如果我们在开始训练时为网络选取更好的先验值，我们使网络更加容易地学习预测好的检测结果。

  - 除了手工选择先验值，我们在训练集边界框上运行 K-均值聚类来自动寻找好的先验值。如果我们使用基于欧式距离的标准 K-均值，较大的框会比较小的框产生更大的误差。然而，我们真正想要的是可以带来好的 ${\rm IOU}$ 分数的先验，这应该是与框的尺寸无关的。因此，我们使用如下方式来度量距离：
    $$
    d({\rm box, centroid}) = 1-{\rm IOU(box, centroid)}
    $$

  - 我们对各种 $k$ 运行 K-均值算法，并绘制平均 ${\rm IOU}$ 与最近中心的图线。我们选择 $k=5$ 作为模型复杂度与高召回之间的折中。聚类中心与手工选择的锚框之间有明显不同。矮胖的锚框很少而瘦高的锚框更多。

  - 我们比较了我们提出的聚类策略和人工选择方法在平均 ${\rm IOU}$ 上的表现。结果表明使用 K-均值来生成边界框使会以更好的表示方式启动模型，使任务更容易学习。

- **直接位置预测（Direct Location Prediction）。** 

  - 在将锚框应用于 YOLO 时我们遇到的第二个问题是模型的不稳定性，尤其是在早期的迭代中。大部分不稳定性来自于对框位置 $(x,y)$ 的预测。在区域推荐网络中，网络预测值 $t_x$ 和 $t_y$ ，中心坐标 $(x,y)$ 通过如下方式计算：
    $$
    \begin{equation}
    \begin{aligned}
    x &= (t_x \times w_a) - x_a \\
    y &= (t_y \times h_a ) - y_a 
    \end{aligned}
    \end{equation}
    $$

  - 例如，预测值 $t_x = 1$ 会使边界框向右移动一个锚框宽度的距离，预测值 $t_x = -1$ 会使边界框向左移动同样的距离。

  - 这个算法没有施加约束，因此锚框位置的预测值最后可能位于图像上的任意位置，与产生该预测的特征位置无关。在使用随机初始化时，模型需要花费很长时间来稳定地预测合理的偏移量。

  - 与区域推荐网络中预测偏移量的方式不同，我们延续 YOLO 的方式，预测相对于网格单元的位置坐标。这将真实值限制在了 $0 -1$ 之间。我们使用一个 logistic 激活函数来限制网络的预测值处于这一区间。

  - 网络对与输出特征映射中的每个单元预测 $5$ 个边界框。网络为每个边界框预测 $5$ 个坐标值，$t_x$ 、$t_y$ 、$t_w$ 、$t_h$ 和 $t_o$ 。如果该单元相对于图像左上角的偏移量为 $(c_x, c_y)$ 并且边界框先验宽度和高度分别为 $p_w$ 和 $p_h$ ，那么对应的预测量为：
    $$
    b_x = \sigma(t_x) + c_x \\
    b_y = \sigma(t_y) + c_y \\
    b_w = p_w e^{t_w} \\
    b_h = p_h e^(t_h) \\
    Pr({\rm object}) \times IOU(b, {\rm object}) = \sigma(t_o)
    $$

  - 由于我们限制了位置预测，这种参数化表达更加容易学习，使得网络更加稳定。使用维度聚类和直接位置预测的网络比直接使用锚框的网络精度提高了接近 $5\%$ 。

- **细粒度特征（Fine-Grained Features）。**

  - 改进的 YOLO 直接在 $13 \times 13$ 的特征映射上预测检测结果。尽管这对于较大目标来说已经足够了，使用细粒度特征可能对于定位较小目标带来帮助。Faster R-CNN 和 SSD 都在一系列特征映射上运行区域推荐网络来获得一系列分辨率。我们使用一种不同的方法，简单地增加一个直通层，从之间的 $26 \times 26$ 分辨率的层中带来特征。
  - 直通层通过将相邻要素堆叠到不同的通道而不是空间位置来将较高分辨率的特征与低分辨率的特征连接在一起。这将特征映射由 $26 \times 26 \times 512$ 转变为 $13 \times 13 \times 2048$ ，转变后的特征映射可以与原来的直接串联。我们的检测器运行在扩展的特征映射上，因而可以使用细粒度特征。这个模型带来了 $1\%$ 的性能提升。

- **多规模训练（Multi-Scale Training）。** 

  - 最初的 YOLO 使用的输入分辨率为 $448 \times 448$ 。引入锚框后我们将分辨率调整到了 $416 \times 416$ 。然而，因为模型中仅含有卷积层和池化层，模型可以动态调整大小。我们希望 YOLOv2 能够对不同大小的输入图像具有鲁棒性，因此我们进行了多规模训练。
  - 我们没有固定输入图像的大小，而是在每几次迭代后改变网络。每 $10$ 批样本我们的网络随机选择一个新的输入图片大小。由于模型的降采样因子为 $32$ ，我们从 $32$ 的下列倍数中选取数值作为输入图片的大小：$\{ 320, 352,...,608\}$ 。因此输入图片的最小分辨率和最大分辨率分别为 $320 \times 320$ 和 $608 \times 608$ 。我们将输入图片大小随机修改为上述数值中的一个，继续进行训练。
  - 该制度迫使网络学习如何在各种输入维度上做好预测。这意味着相同的网络可以在不同的输入分辨率下给出检测结果的预测值。该网络在更小的尺寸下运行速度更快，因此 YOLOv2 给出了速度和精度间的简单折中。
  - 在低分辨率下，YOLOv2 运行速度快，精度也更高。在 $288 \times 288$ 的输入分辨率下，可以达到 $90 \ {\rm FPS}$ 的运行速度，精度几乎与 Faster R-CNN 一样好。对于低性能 GPU 、高帧率视频或者多路视频流来说是很理想的。
  - 在高分辨率下，YOLOv2 是一个先进的检测器，在 VOC 2007 数据集上达到了 $78.6 {\rm mAP}$ 检测精度，运行速度仍满足实时要求。

## 3 更快（Faster）

- 我们想要检测更精确，但也想检测速度更快。大多数检测的应用，例如机器人或者自动驾驶汽车，依赖低延迟的预测。为了最大限度的提高性能，我们在设计之初就考虑如何让 YOLOv2 更加快速。
- 大多数检测框架将 VGG-16 作为基础的特征提取器。VGG-16 是一个强力、精确的分类网络，但是具有不必要的复杂性。对于一张分辨率为 $224 \times 224$ 图片，VGG-16 网络的卷积层需要进行 $3.069 \times 10^{10}$ 次浮点运算来完成一次前向传播。
- YOLO 框架使用一个基于 GoogLeNet 架构的自定义网络。这个网络比 VGG-16 网络更快，一次前向传播仅需要 $8.52 \times 10^9$ 次浮点运算。然而，它的精度略逊于 VGG-16 。在 ImageNet 数据集上，YOLO 自定义模型的精度为 $88 \%$ ，VGG-16 的精度为 $90 \%$ 。
- **DarkNet-19。** 
  - 我们提出了一个新的分类模型来用作 YOLOv2 的基础。该网络建立在前人在网络设计上的工作和该领域的常用知识上。与 VGG-16 网络类似，我们大部分使用 $3 \times 3$ 卷积核并且在每次池化步骤之后倍增通道数。根据 Network in Network （NIN）的工作，我们使用全局平均池化来作出预测，并且在 $3 \times 3$ 卷积层之间使用 $1 \times 1$ 卷积核来压缩特征表达。我们使用批量标准化来增加训练的稳定性，加速收敛和规范模型。
  - 我们最终的模型称为 Darknet-19 ，包含 $19$ 个卷积层和 $5$ 个最大池化层。该模型仅需要 $5.58 \times 10^9$ 次浮点运算来处理一张图像，已经达到了 ImageNet 数据集中 top-1 精度的 $72.9 \%$ ，top-5 精度的 $91.2 \%$ 。
- **分类训练（Training for Classification）。** 
  - 我们在标准的 ImageNet 1000 种类的分类数据集上，使用随机梯度下降进行了 $160$ 轮的训练，起始的学习速率为 $0.1$ ，进行 $4$ 次多项式速率的衰减，使用 $0.0005$ 的权重衰减，和 $0.9$ 的动量，使用 Darknet 神经网络框架。在训练过程中，我们使用标准的数据增加技巧，包括随机修剪（crop）、旋转、色调、饱和度和曝光度的偏移。
  - 如上文所述，在分辨率为 $224 \times 224$ 的图片上进行初始训练后，我们在更高的图片分辨率 $448 \times 448$ 下进行网络的微调。在微调过程中，我们对上述参数进行 $10$ 轮的训练，起始的学习速率为 $10^{-3}$ 。在高分辨率下，我们的网络达到了 top-1 精度的 $76.5 \%$ ，和 top-5 精度的 $93.3 \%$ 。
- **检测训练（Training for Detection）。** 
  - 我们修改网络用于检测，去掉最后的卷积层，作为替代加入 $3$ 个带有 $1024$ 个滤波器的卷积层，然后是最后的 $1 \times 1$ 卷积层，其中包含我们需要用于的检测的输出数量。对于 VOC 数据集，我们为 $5$ 个边界框中的每一个预测 $5$ 个坐标值和 $20$ 个类标签概率，因此是一个 $125$ 维的滤波器。我们还增加了一个从最后的 $3 \times 3 \times 512$ 层到倒数第二个卷积层的直通层，因此我们的模型可以使用更好的细粒度特征。
  - 我们对模型进行了 $160$ 轮的训练，起始的学习速率为 $10^{-3}$ ，在第 $60$ 轮和第 $90$ 轮时将学习速率再除以 $10$ 。我们使用 $0.0005$ 的权重衰减和 $0.9$ 的动量。我们使用与 YOLO 和 SSD 相似的数据增加方法，比如随机修剪、颜色偏移等。我们在 COCO 和 VOC 上使用相同的学习策略。

## 4 更强（Stronger）

