# 第5章 决策树

- 概述
  - 基本的分类与回归方法。
  - 模型呈树形结构。
  - 优点：
    - 具有可读性；
    - 分类速度快。
  - 学习步骤：
    - 特征选择；
    - 决策树的生成；
    - 决策树的修建。

## 5.1 决策树模型与学习

### 5.1.1 决策树模型

- 分类决策树模型是一种描述对实例进行分类的树形结构，由结点和有向边组成。
  - 结点
    - 内部结点：表示一个特征或属性；
    - 叶结点：表示一个类。
  - 分类过程：
    - 从根结点开始，对实例的某一特征进行测试，根据测试结果将该实例分配到子结点；
    - 每个子结点对应着该特征的一个取值；
    - 递归地对实例进行测试和分配，直至到达叶结点；
    - 最后将实例分到叶节点的类中。

### 5.1.2 决策树与 if-then 规则

- 由**决策树的根结点到叶结点的每一条路径构**建一条**规则**；
- 路径上内部结点的特征对应着规则的条件，叶结点的类对应着规则的结论；
- 决策树的路径或其对应的 if-then 规则集合具有互斥性和完备性。

### 5.1.3 决策树与条件概率分布

- 决策树可以表示**给定特征**条件下**类的条件概率分布**。
  - 将特征空间划分为互不相交的单元或区域，决策树的一条路径对应于划分中的一个单元；
  - 在每个单元上定义一个类的概率分布，从而构成一个条件概率分布；
  - 决策树所表示的条件概率分布由各个单元在给定条件下类的条件概率分布组成；
  - 各叶结点（单元）上的条件概率往往偏向某一个类，决策树分类时将该叶结点的实例强行归到条件概率大的那一类上。

### 5.1.4 决策树学习

- 目标：根据给定的训练数据集构建一个决策树模型，使其能够对实例进行正确的分类。
- 本质：从训练数据集中归纳出一组分类规则。
- 模型：
  - 与训练数据集不相矛盾（即能对训练数据进行正确分类）的决策树可能有多个，也可能一个也没有。
  - 理想的模型是一个与训练数据矛盾较小，同时具有很好的泛化能力的决策树。
- 损失函数：
  - 决策树学习的损失函数通常是正则化的极大似然函数。
  - 决策树学习的策略是以损失函数为目标函数的最小化。
- 学习算法：
  - 决策树学习算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。
  - 学习过程对应着对特征空间的划分，也对应着决策树的构建。
  - 具体过程：
    - 构建根节点，将所有训练数据都放在根节点；
    - 选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类；
      - 如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；
      - 如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的节点；
    - 如此递归进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止；
    - 最后每个子集都被分到叶结点上（即都有了明确的类），这就生成了一棵决策树。
- 算法分析：
  - 以上方法生成的决策树可能发生过拟合现象，需要自下而上进行剪枝，使其变得更简单，具有更好的泛化能力；
  - 具体地，就是去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点；
  - 如果特征数量很多，也可以在决策树开始学习的时候，对特征进行选择，只留下对训练数据有足够分类能力的特征。
- 总结：
  - 决策树的生成只考虑局部最优，而决策树的剪枝则考虑全局最优。
  - 深浅不同的决策树对应着不同复杂度的模型。

## 5.2 特征选择

### 5.2.1 特征选择问题

- 目的：选取对于训练数据具有**分类能力**的特征，以提高决策树学习的效率。
- 准则：信息增益或信息增益比。

### 5.2.2 信息增益 

- 熵（表示随机变量不确定性的度量）

  - 定义：

    设 $X$ 是一个取有限个值的离散随机变量，其概率分布为
    $$
    P(X = x_i) = p_i, \ \ i = 1,2, \dots,n
    $$
    则随机变量 $X$ 的熵定义为
    $$
    H(X) = -\sum_{i=1}^{n} p_i \ \text{log} \ p_i
    $$

  - 注：

    - 若 $p_i = 0$ ，则定义 $0 \ \text{log} \ 0 = 0$ ；
    - 式中的对数以 $2$ 为底或以 $e$ 为底，这时熵的单位分别为比特（bit）或纳特（nat）；
    - 熵只依赖于随机变量 $X$ 的分布，而与 $X$ 的取值无关；
    - 熵越大，随机变量 $X$ 的不确定性越大。

- 条件熵（表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性）

  - 定义：

    设有随机变量 $(X, Y)$ ，其联合概率分布为
    $$
    P(X = x_i, Y = y_j) = p_{ij}, \ \ i = 1,2,\dots,n; \ \ j = 1,2,\dots,m
    $$
    则随机变量 $X$ 给定的条件下随机变量 $Y$ 的条件熵定义为
    $$
    H(Y|X) =\sum_{i=1}^{n} p_i H(Y|X = x_i)= - \sum_{i=1}^{n} p_i \ p_{ij} \text{log} \ p_{ij}
    $$

  - 注：

    - 条件熵相当于 $X$ 给定条件下 $Y$ 的条件概率分布的熵对 $X$ 的数学期望；
    - 当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验上和经验条件熵。

- 信息增益（表示由于特征 $A$ 而使得对数据集 $D$ 的分类的不确定性减少的程度）

  - 定义：

    特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D, A)$ ，定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D|A)$ 之差，即
    $$
    g(D, A) = H(D) - H(D|A)
    $$

  -  注：

    - 对于数据集 $D$ 而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力。

  - 信息增益的数据估计：

    - 已知条件

      训练数据集 $D$ ，样本容量为 $|D|$ ；

      训练数据中包含 $K$ 个类 $C_k$ （$k = 1,2,\dots,K$ ），属于类 $C_k$ 的样本个数为 $|C_k|$ ；

      特征 $A$ 有 $n$ 个不同的取值 $\{ a_1, a_2, \dots ,a_n \}$ ，根据特征 $A$ 的取值将数据 $D$ 划分为 $n$ 个子集 $D_i$ （$i = 1,2, \dots, n$），子集 $D_i$ 中的样本个数为 $|D_i|$ ；

      子集 $D_i$ 中属于类 $C_k$ 的样本的集合为 $D_{ik}$ ，其中包含的样本个数为 $|D_{ik}|$ 。

    - 计算数据集 $D$ 的经验熵 $H(D)$ 
      $$
      H(D) = - \sum_{k=1}^{K} \frac{|C_k|}{|D|} \text{log}  \frac{|C_k|}{|D|} 
      $$

    - 计算特征 $A$ 对数据集 $D$ 的经验熵 $H(D|A)$ 
      $$
      H(D|A) = \sum_{i=1}^{n} \frac{|D_i|}{|D|} H(D_i)
      = - \sum_{i=1}^{n}  \frac{|D_i|}{|D|} \sum_{k =1}^{K} \frac{|D_{ik}|}{|D_i|} \text{log} \frac{|D_{ik}|}{|D_i|}
      $$

    - 计算信息增益 $g(D, A)$ 
      $$
      g(D, A) = H(D) - H(D|A) 
      $$

- 信息增益比

  - 定义：

    特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D, A) $ 定义为其信息增益 $g(D, A)$ 与训练数据集 $D$ 关于特征 $A$ 的熵 $H_A(D)$ 之比，即
    $$
    g_R(D, A) = \frac{g(D, A)}{H_A(D)}
    $$
    其中， $H_A(D)$ 的表达式为
    $$
    H_A(D) = - \sum_{i=1}^{n} \frac{|D_i|}{|D|} \text{log} \frac{|D_i|}{|D|}
    $$

  - 注：

    - 以信息增益为准则选取划分数据集的特征，存在偏向与选择取值较多的特征的问题，使用信息增益比可以对之一问题进行校正。

## 5.3 决策树的生成

### 5.3.1 ID3  算法

- 过程：
  - 从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；
  - 再对子结点递归地调用以上方法，构建决策树；
  - 直到所有特征的信息增益均很小或没有特征可以选择为止；
  - 最终得到一个决策树。
- 注：
  - ID3 算法相当于用极大似然法进行概率模型的选择。

### 5.3.2 C4.5的生成算法

- 与ID3 算法类似，使用信息增益比来选择特征。

## 5.4 决策树的剪枝

- 目的：（解决过拟合问题）

  - 决策树的生成过程中过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树，以至于对未知数据的分类不够准确。
  - 决策树的剪枝考虑决策树的复杂度，对已生成的决策树进行简化。

- 原理：

  - 通过极小化决策树整体的损失函数来实现。

- 损失函数：

  - 定义：

    设树 $T$ 的叶结点数为 $|T|$ ，$t$ 是树 $T$ 的叶结点，该叶结点有 $N_t$ 个样本点，其中属于类 $C_k$ 的样本点有 $N_{tk}$ 个，$H_t(T)$ 为叶结点 $t$ 上的经验熵， $\alpha \ge 0$ 为参数，则决策树的损失函数可以定义为
    $$
    \begin{aligned}
    C_{\alpha}(T) &= \sum_{i=1}^{|T|} N_t H_t(T) + \alpha |T| \\ 
    &=  - \sum_{i=1}^{|T|} \sum_{k=1}^{K} N_{tk} \ \text{log} \frac{N_{tk}}{N_t}+ \alpha |T|
    \end{aligned}
    $$
    将上式右端第一项记作 $C(T)$ ，则
    $$
    C_{\alpha}(T) = C(T) + \alpha |T|
    $$
    其中，$C(T)$ 表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$ 表示模型复杂度，参数 $\alpha$ 控制二者之间的权重，较大的 $\alpha$ 促使选择较为简单的模型（树），较小的 $\alpha$ 促使选择较为复杂的模型（树）。

  - 注：

    - 损失函数实现了对模型与训练数据的拟合程度和模型本身复杂度二者之间的平衡。

- 生成与剪枝之间的关系：

  - 决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合；
  - 决策树剪枝通过优化损失函数，还考虑了减小模型整体复杂度；
  - 决策树生成学习局部的模型，决策树剪枝学习整体的模型。

- 剪枝算法：

  - 计算每个节点的经验熵；
  - 递归地从树的叶结点向上回缩，如果回缩后的损失函数更小，则进行剪枝，即将父节点变为新的叶结点；
  - 返回上一步，直至不能继续为止，得到损失函数最小的子树。

## 5.5 CART 算法

- 概述：
  - 分类与回归树（CART）是应用广泛的决策树学习方法，既可用于分类也可用于回归；
  - CART 假设决策树为二叉树，内部结点特征的取值为“是”或“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支；
  - 这样的决策树递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的概率分布。

### 5.5.1 CART 生成

- 概述：

  - 决策树的生成就是递归地构建二叉决策树的过程；
  - 对于回归树使用平方误差最小化准则，对于分类树使用基尼指数最小化准则，进行特征选择。

- 回归树的生成：

  - 回归树模型的定义：

    一个回归树对应着输入空间（特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入控件划分为 $M$ 个单元 $R_1, R_2, \dots, R_M$ ，并且在每个单元 $R_m$ 上有一个固定的输出值 $c_m$ ，于是回归树模型可以表示为
    $$
    f(x) = \sum_{m=1}^M c_m I(x \in R_m)
    $$
    当输入空间的划分确定时，可以采用平方误差来表示回归树对于训练数据的预测误差，用平方误差最小准则求解每个单元上的最优输出值。易知，单元 $R_m$ 上的 $c_m$ 的最优值 $\hat {c}_m$ 是 $R_m$ 上的所有输入实例 $x_i$ 对应的输出 $y_i$ 的均值，即
    $$
    \hat c_m = \text{ave} (y_i | x_i \in R_m)
    $$

  - 输入空间的划分

    采用启发式的方法：

    选择第 $j$ 个变量 $x^{(j)}$ 和它取的值 $s$ ，作为切分变量和切分点，并定义两个区域
    $$
    R_1(j, s) = \{ x|x^{(j)} \le s\} \\ R_2(j, s) =  \{ x|x^{(j)} > s\}
    $$
    然后寻找最优切分变量 $j$ 和最优切分点 $s$ ，具体地，求解
    $$
    \min_{j,s} \bigg[ \min_{c_1} \sum_{x_i \in R_1(j, s)} (y_i - c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i - c_2)^2 \bigg]
    $$
    对固定输入变量 $j$ 可以找到最优切分点 $s$ 。
    $$
    \hat c_1 = \text{ave} (y_i | x_i \in R_1(j,s )) \\
    \hat c_2 = \text{ave} (y_i | x_i \in R_2(j, s))
    $$

  - 回归树生成算法：

    - 遍历所有输入变量，找到最优切分变量 $j$ ，构成一个对 $(j, s)$ ，依次将输入空间划分为两个区域；
    - 对每个区域重复上述划分过程，直到满足停止条件为止；
    - 最后，得到一个最小二乘回归树。

- 分类树的生成：

  - 基尼指数：

    分类问题中，假设有 $K$ 个类，样本点属于第 $k$ 类的概率为 $p_k$ ，则概率分布的基尼指数定义为
    $$
    \text{Gini} (p) = \sum_{k=1}^K p_k(1 - p_k) = 1-\sum_{k=1}^K p_k^2
    $$
    对于给定的样本集合 $D$ ，其基尼指数为
    $$
    \text{Gini} (D) = 1-\sum_{k=1}^K \bigg( \frac{|C_k|}{|D|} \bigg)^2
    $$
    如果样本集合 $D$ 根据特征 $A$ 是否取某一可能值 $a$ 被分割成 $D_1$ 和 $D_2$ 两部分，则在特征 $A$ 的条件下，集合 $D$ 的基尼指数定义为
    $$
    \text{Gini} (D, A) = \frac{|D_1|}{|D|} \text{Gini} (D_1) + \frac{|D_2|}{|D|} \text{Gini} (D_2)
    $$
    基尼指数 $\text{Gini}(D)$ 表示集合 $D$ 的不确定性，基尼指数 $\text{Gini}(D,A)$ 表示经 $A=a$ 分割后集合 $D$ 的不确定性，基尼指数值越大，样本集合的不确定性也就越大（与熵类似）。

  - 分类树生成算法：

    - 设结点的的训练数据集为 $D$ ，计算现有特征对该数据集的基尼指数；
    - 在所有可能的特征 $A$ 以及它们所有的可能的切分点 $a$ 中，选择基尼指数最小的特征以及对应的切分点作为最优特征和最优切分点；
    - 依照最优特征和最优切分点， 从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中；
    - 对两个子结点递归地进行上述切分过程，直至满足停止条件；
    - 最后，得到一个CART分类树。

### 5.5.2 CART 剪枝

- 步骤：
  - 首先从生成算法产生的决策树 $T_0 $ 的底端开始不断剪枝，直到 $T_0 $ 的根结点，形成一个子树序列 $\{ T_0, T_1, \dots, T_n\}$；
  - 然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。

