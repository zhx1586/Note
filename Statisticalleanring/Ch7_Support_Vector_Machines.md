# 第7章 支持向量机

- 概述：
  - 二分类模型；
  - 基本模型是定义在特征空间上的间隔最大的线性分类器；
  - 通过核技巧使其成为实质上的非线性分类器；
  - 学习策略为间隔最大化；
  - 学习算法为求解凸二次规划的最优化算法。
- 分类（由简至繁）：
  - 线性可分支持向量机（训练数据线性可分，硬间隔最大化）；
  - 线性支持向量机（训练数据近似线性可分，软间隔最大化）；
  - 非线性支持向量机（线性不可分，核技巧+软间隔最大化）。
- 核函数：
  - 定义：当输入控件为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。
  - 作用：通过使用核函数可以学习非线性支持向量机，等于隐式地在高维的特征空间中学习线性支持向量机。

## 7.1 线性可分支持向量机与硬间隔最大化

### 7.1.1 线性可分支持向量机

- 前提：

  - 二分类问题；
  - 输入空间与特征空间为两个不同的空间；
  - 输入空间为欧式空间或离散集合；
  - 特征空间为欧式空间或希尔伯特空间。

- 思想：

  - 线性可分支持向量机、线性支持向量机假设输入空间与特征空间中的元素一一对应，并将输入空间中的**输入**映射为特征空间中的**特征向量**。
  - 非线性支持向量机利用一个从输入空间到特征空间的非线性映射将**输入**映射为**特征向量**。
  - 支持向量机的学习在特征空间中进行。

- 学习目标：

  - 在特征空间中找到一个分离超平面，能将实例分到不同的类。

- 几何解释：

  - 分离超平面对应于方程 $w \cdot x+b=0 $ ，它由法向量 $w$ 和截距 $b$ 决定，可用 $(w, b)$ 来表示；
  - 分离超平面将特征空间划分为两部分，法向量指向的一侧为正类，另一侧为负类；

- 与感知机的区别与联系：

  - 一般地，当训练数据集线性可分时，存在无穷多个分离超平面可将两类数据正确分开；
  - 感知机采用误分类最小策略，求得分离超平面，不过这时的解有无穷多个；
  - 线性可分支持向量机采用间隔最大化求最优分离超平面，此时解是唯一的。

- 定义：

  给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为
  $$
  w^* \cdot x +b^* = 0
  $$
  以及相应的分类决策函数
  $$
  f(x) = \text{sign} (w^* \cdot x + b^*)
  $$
  称为线性可分支持向量机。

### 7.1.2 函数间隔和几何间隔

- 函数间隔：

  - 定义：

    对于给定的训练数据集 $T$ 和超平面 $(w, b)$ ，定义超平面 $(w, b)$ 关于样本点 $(x_i, y_i)$ 的函数间隔为
    $$
    \hat\gamma_i = y_i (w \cdot x_i +b)
    $$
    定义超平面 $(w, b)$ 关于训练数据集 $T$ 的函数间隔为超平面 $(w, b)$ 关于 $T$ 中所有样本点 $(x_i, y_i)$ 的函数间隔的最小值，即
    $$
    \hat\gamma = \min_{i=1,\dots, N} \hat\gamma_i
    $$

  - 含义：

    函数间隔表示分类预测的正确性及确信度。

- 几何间隔：

  - 定义：

    对于给定的训练数据集 $T$ 和超平面 $(w, b)$ ，定义超平面 $(w, b)$ 关于样本点 $(x_i, y_i)$ 的几何间隔为
    $$
    \gamma_i = y_i \bigg( \frac{w}{\|w\|} \cdot x_i + \frac{b}{\|w\|}  \bigg)
    $$
    定义超平面 $(w, b)$ 关于训练数据集 $T$ 的函数间隔为超平面 $(w, b)$ 关于 $T$ 中所有样本点 $(x_i, y_i)$ 的函数间隔的最小值，即
    $$
    \gamma = \min_{i=1, \dots, N} \gamma_i
    $$

  - 含义：

    超平面 $(w, b)$ 关于样本点 $(x_i, y_i)$ 的几何间隔一般是实例点到超平面的带符号的距离，当样本点被超平面正确分类时，就是实例点到超平面的距离。


- 函数间隔与几何间隔之间的关系

  - 关系：
    $$
    \gamma_i = \frac{\hat\gamma_i}{\|w\|} \\
    \gamma = \frac{\hat\gamma}{\|w\|}
    $$

  - 特点：

    如果超平面参数 $w$ 和 $b$ 成比例地改变（超平面没有变化）

    - 函数间隔也按此比例改变；
    - 几何间隔保持不变。

### 7.1.3 间隔最大化

- 直观解释：

  对训练数据集找到几何间隔最大的分类超平面意味着以充分大的确信度对训练数据进行分类，也就是说，不仅将正负实例点分开，而且对最难区分的实例点（离超平面最近的点）也有足够大的确信度将它们分开，这样的分类超平面应该对未知的新实例有很好的分类预测能力。

- 实现：

  将求解最大几何间隔的分离超平面表示为以下的约束最优化问题
  $$
  \begin{aligned}
  & \max_{w, b} \gamma \\ 
  &\ \text{s.t.} \ \ y_i \bigg( \frac{w}{\|w\|} \cdot x_i + \frac{b}{\|w\|} \bigg) \ge \gamma , \ \ i= 1,2, \dots,N
  \end{aligned}
  $$
  即我们希望最大化超平面 $(w, b)$ 关于训练数据集的几何间隔 $\gamma$ ，约束条件表示的是超平面 $(w, b)$ 关于每个样本点的几何间隔至少是 $\gamma$ 。

- 进一步转化：

  - 考虑到几何间隔和函数间隔的关系，可将该问题改写为
    $$
    \begin{aligned}
    & \max_{w,b} \frac{\hat\gamma}{\|w\|} \\
    & \ \text{s.t.} \ \  y_i \big( w \cdot x_i + b \big) \ge \hat\gamma, \ \ i = 1,2,\dots,N
    \end{aligned}
    $$

  - 函数间隔 $\hat\gamma$ 的取值不影响最优化问题的解，因此取 $\hat\gamma = 1$ ，而且最大化 $\frac{1}{\|w\|}$ 和最小化 $\frac{1}{2} \|w\|^2$ 是等价的，可将问题进一步改写为
    $$
    \begin{aligned}
    & \min_{w, b} \frac{1}{2} \|w\|^2 \\
    & \ \text{s.t.} \ \ y_i (w \cdot x+b) -1 \ge 0, \ \ i = 1, 2, \dots, N
    \end{aligned}
    $$
    最后得到了线性可分支持向量机学习的最优化问题，这是一个凸二次规划问题。

- 支持向量和间隔边界

  - 支持向量：

    在线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量。支持向量是使约束条件使等号成立的点。
    $$
    y_i (w \cdot x_i + b) - 1 = 0
    $$

  - 间隔边界：

    对于 $y_i = +1$ 的正例点，支持向量在超平面 $H_1: w \cdot x+b = 1$ 上，对于 $y_i = -1$ 的负例点，支持向量在超平面 $H_2 : y_i (w \cdot x_i +b) = -1$ 上。$H_1$ 与 $H_2$ 之间的距离称为间隔，间隔依赖于分类超平面的法向量 $w$ ，等于 $\frac{2}{\|w\|}$ 。$H_1$ 和 $H_2$ 称为间隔边界。

### 7.1.4 学习的对偶算法

- 优点：

  - 对偶问题往往更容易求解；
  - 自然引入核函数，进而推广到非线性分类问题。

- 算法：

  - 首先，构建拉格朗日函数。

    对每一个不等式约束 $y_i (w \cdot x+b) -1 \ge 0$ 引进拉格朗日乘子 $\alpha_i \ge 0$ ，定义拉格朗日函数
    $$
    L(w, b, a) = \frac{1}{2} \|w\|^2 + \sum_{i=1}^N \alpha_i [1 - y_i (w \cdot x_i + b)]
    $$
    其中，$\alpha_i = (\alpha_1, \alpha_2, \dots, \alpha_N) ^T$ 为拉格朗日乘子向量。 

    根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：
    $$
    \max_{\alpha} \ \min_{w, b} L(w, b, \alpha)
    $$
    所以为了得到对偶问题的解，需要先求 $L(w, b, \alpha)$ 对 $w$ 、$b$ 的极小，再求对 $\alpha$ 的极大。

    - 求 $L(w, b, \alpha)$ 对 $w$ 、$b$ 的极小

      将拉格朗日函数 $L(w, b, \alpha)$ 分别对 $w$ 、$b$ 求偏导数并令其等于 $0$ 
      $$
      \nabla_w L(w, b, \alpha) = w - \sum_{i=1}^N \alpha_i y_i x_i = 0 \\
      \nabla_b L(w, b, \alpha) = -\sum_{i=1}^N \alpha_i y_i = 0
      $$
      可得
      $$
      w = \sum_{i=1}^N \alpha_i y_i x_i \\
      \sum_{i=1}^N \alpha_i y_i = 0
      $$
      将上式代入拉格朗日函数可得
      $$
      \begin{aligned}
      L(w, b, \alpha) &= \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) 
      - \sum_{i=1}^N \alpha_i y_i \bigg(\bigg(\sum_{j=1}^N \alpha_j y_j x_j \bigg) \cdot x_i + b \bigg)
      + \sum_{i=1}^N \alpha_i \\
      &=  -\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) + \sum_{i=1}^N \alpha_i
      \end{aligned}
      $$
      即
      $$
      \min_{w, b} L(w, b, \alpha) =
      -\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) + \sum_{i=1}^N \alpha_i
      $$

    - 求 $L(w, b, \alpha)$ 对 $\alpha$ 的极大
      $$
      \begin{aligned}
      & \max_{\alpha} -\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) 
      + \sum_{i=1}^N \alpha_i \\
      & \ \text{s.t.} \ \ \sum_{i=1}^N \alpha_i y_i = 0 \\
      & \ \qquad \alpha_i \ge 0, \ \ i = 1, 2, \dots, N
      \end{aligned}
      $$
      将上述问题的目标函数求对 $\alpha_i $ 的偏导数并令其等于 $0$ 即可求得最优的拉格朗日乘子向量 $\alpha^* = \{ \alpha_1^*, \alpha_2^*, \dots, \alpha_N^* \}$ 。

      之后计算
      $$
      w^* = \sum_{i=1}^N \alpha_i^* y_i x_i
      $$
      并选择 $\alpha^*$ 的一个正分量 $\alpha_j > 0$ ，计算
      $$
      b^* = y_j - \sum_{i=1}^N \alpha_i^* y_i (x_i \cdot x_j)
      $$
      从而求得分类超平面
      $$
      w^* \cdot x + b^* = 0
      $$
      以及分类决策函数
      $$
      f(x) = \text{sign} (w^* \cdot x + b^*)
      $$

- 支持向量

  - 定义：

    将训练数据集中对应于 $\alpha_i^* > 0$ 的样本点 $(x_i, y_i)$ 的实例 $x_i \in \text{R}^n$ 称为支持向量。

  - 性质：

    对于线性可分支持向量机，支持向量 $x_i$ 一定在间隔边界上。

## 7.2 线性支持向量机与软间隔最大化

### 7.2.1 线性支持向量机

- 软间隔最大化

  - 思想：

    线性不可分意味着某些样本点 $(x_i, y_i)$ 不能满足函数间隔不大于等于 $1$ 的约束条件，为了解决这个问题，可以对每个样本点 $(x_i, y_i)$ 引进一个松弛变量 $\xi_i \ge 0$ ，使函数间隔加上松弛变量大于等于 $1$ 。这样，约束条件变为
    $$
    y_i (w \cdot x_i +b) \ge 1 - \xi_i
    $$
    同时，对每个松弛变量 $\xi_i$ ，支付一个代价 $\xi_i$ ，目标函数由原来的 $\frac{1}{2} \|w\| ^ 2$ 变成
    $$
    \frac {1}{2} \|w\|^2 + C \sum_{i=1}^N \xi_i
    $$
    其中，$C>0$ 称为惩罚参数，一般由应用问题觉得，$C$ 值越大，对于误分类的惩罚越大。

  - 含义：

    - 使间隔尽可能大；
    - 使误分类点的个数尽量小。

  - 意义：

    - 现实中的数据集往往是线性不可分的，线性支持向量机具有更广的普适性。

### 7.2.2 学习的对偶算法

- 算法：

  - 选择惩罚参数 $C>0$ ，构造并求解凸二次规划问题
    $$
    \begin{aligned}
    & \min_{\alpha} \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i, y_j (x_i \cdot  x_j) 
    - \sum_{i=1}^N \alpha_i \\
    & \ \text{s.t.} \ \sum_{i=1}^N \alpha_i y_i = 0 \\
    & \qquad 0 \le \alpha_i \le C , \ \ i = 1,2, \dots, N
    \end{aligned}
    $$
    求得最优解 $\alpha^* = (\alpha_1^*, \alpha_2^*, \dots, \alpha_N^*)^T$ 。

  - 计算 $w^*$ 、$b^*$ 
    $$
    w^* = \sum_{i=1}^N \alpha^* y_i x_i
    $$
    选择 $\alpha^*$ 的一个 $\alpha_j^*$ 分量适合条件 $0 < \alpha_j^* < C$ ，计算
    $$
    b^* = y_j - \sum_{i=1}^N y_i \alpha^* (x_i \cdot x_j)
    $$

  - 求得分类超平面

  $$
  w^* \cdot x + b^* = 0
  $$

    	以及分类决策函数

  $$
  f(x) = \text{sign} (w^* \cdot x + b^*)
  $$



### 7.2.3 支持向量

- 软间隔的支持向量 $x_i$ 
  - 或者在间隔边界上（$\xi_i = 0 $）；
  - 或者在间隔边界与分离超平面之间（$0 < \xi_i < 1$）；
  - 或者在分离超平面的误分类一侧（$\xi_i > 1$）。

### 7.2.4 合页损失函数

- 定义：

  线性支持向量机学习还有另外一种解释，就是最小化以下目标函数
  $$
  \sum_{i=1}^N [1-y_i (w \cdot x_i + b) ]_{+} + \lambda \|w\|^2
  $$
  目标函数的第一项是经验损失或者经验风险，函数
  $$
  L(y(w \cdot x + b)) = [1 - y_i (w \cdot x + b)]_{+}
  $$
  称为合页损失函数。下标“+”表示以下取正值的函数
  $$
  [z]_{+} = 
  \left\{
  \begin{aligned}
  z, \qquad z >0 \\
  0, \qquad z \le 0
  \end{aligned}
  \right.
  $$

- 含义：

  - 当样本点 $(x_i, y_i)$ 被正确分类且函数间隔（置信度）$y_i (w \cdot x_i + b)$ 大于 $1$ 时，损失是 $0$ ，否则损失是 $1 - y_i (w \cdot x_i + b)$ 。
  - 目标函数的第二项系数为 $\lambda$ 的 $w$ 的 $L_2$ 范数，是正则化项。
  - 可以认为 $0-1$ 损失函数是二分类问题真正的损失函数，而合页损失函数是其上界，由于 $0-1$ 损失函数不是连续可导的，直接优化其构成的目标函数比较困难，因此线性支持向量机选择优化由 $0-1$ 损失函数的上界（合页损失函数）构成的目标函数，这时的上界损失函数又称为代理损失函数。
  - 相比于感知机的损失函数 $[- y_i (w \cdot x_i + b)]_{+}$ ，合页损失函数不仅要分类正确，而且置信度足够高时损失才是 $0$ ，对学习有更高的要求。 

## 7.3 非线性支持向量机与核函数

### 7.3.1 核技巧

- 非线性分类问题：

  - 非线性分类问题是指通过利用非线性模型才能很好地进行分类的问题。
  - 如果能用 $\text{R}^n$ 中的一个超曲面将正负实例正确分开，则称这个问题为非线性可分问题。

- 用线性分类方法求解非线性问题：

  - 首先使用一个变换将原空间的数据映射到新空间；
  - 然后在新空间里用线性分类学习方法从训练数据中学习分类模型。

- 核函数

  - 定义：

    设 $ \mathcal{X} $ 是输入空间（欧式空间 $\text{R}^n$ 的子集或离散集合），又设 $ \mathcal{H} $ 为特征空间（希尔伯特空间），如果存在一个从 $\mathcal{X}$ 到 $\mathcal{H}$ 的映射
    $$
    \phi(x) : \mathcal{X} \rightarrow \mathcal{H}
    $$
    使得对所有 $x, z \in \mathcal{X} $ ，函数 $K(x, z)$ 满足条件
    $$
    K(x, z) = \phi(x) \cdot \phi(z)
    $$
    则称 $K(x, z)$ 为核函数，$\phi(x)$ 为映射函数，式中 $\phi(x) \cdot \phi(z)$ 为 $\phi(x)$ 和 $\phi(z)$ 的内积。

  - 注：

    - 在学习和预测中只定义核函数 $K(x, z)$ ，而不显示地定义映射函数 $\phi$ 。
    - 通常，直接计算 $K(x, z)$ 比较容易，而通过 $\phi(x)$ 和 $\phi(z)$ 计算 $K(x, z)$ 并不容易。
    - 特征空间 $\mathcal{H}$ 一般是高维的，甚至是无穷维的。
    - 对于给定的核函数 $K(x, z)$ ，特征空间 $\mathcal{H}$ 和映射函数 $\phi$ 的取法并不唯一，可以取不同的特征空间，即便在同一特征空间里也可以取不同的映射。

- 核技巧在支持向量机中的应用

  - 算法：

    在线性支持向量机的对偶问题中，无论是目标函数还是决策函数（分离超平面）都只涉及输入实例与实例之间的内积。在对偶问题的目标函数中的内积 $x_i \cdot x_j$ 可以用核函数 $K(x_i, y_i) = \phi(x_i) \cdot \phi(x_j)$ 来代替。此时对偶问题的目标函数成为

  $$
  W(\alpha) = \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j K(x_i, x_j) - \sum_{i=1}^N \alpha_i
  $$

  ​	同样，分类决策函数中的内积也可以用核函数来代替，分类决策函数式成为	
  $$
  \begin{aligned}
  f(x) &= \text{sign}\left(\sum_{i=1}^{N_s}\alpha_i^* y_i \phi(x_i) \cdot \phi(x) + b^* \right) \\
  &= \text{sign}\left(\sum_{i=1}^{N_s}\alpha_i^* y_i K(x_i, x) + b^* \right)
  \end{aligned}
  $$

  - 思想：
    - 上述过程等价于经过映射函数 $\phi$ 将原来的输入空间变换到一个新的特征空间，在新的特征空间里从训练样本中学习支持向量机。
    - 当映射函数为非线性函数时，学习到的含有核函数的支持向量机是非线性模型。
    - 也就是说，在核函数 $K(x, z)$ 给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机，学习是隐式地在特征空间进行的，不需要显示地定义特征空间和映射函数。
    - 在实际应用中，往往依赖领域知识直接选择核函数，核函数选择的有效性需要通过实验验证。

### 7.3.2 正定核

- 通常所说的核函数就是正定核函数。

- 正定核的充要条件

  设 $K: \mathcal{X} \times \mathcal{X} \rightarrow \text{R}$ 是对称函数，则 $K(x, z)$ 为正定核函数的充要条件是对任意的 $x_i \in \mathcal{X}$ ，$i = 1, 2, \dots, m$ ，$K(x, z)$ 对应的 $\text{Gram}$ 矩阵：
  $$
  K = \left[ K(x_i, x_j) \right]_{m \times m}
  $$
  是半正定矩阵。

### 7.3.3 常用的核函数

- 多项式核函数
  $$
  K(x, z) = (x \cdot z + 1)^p
  $$
  对应的支持向量机是一个 $p$ 次多项式分类器。

- 高斯核函数
  $$
  K(x, z) = \exp \left( - \frac{\|x-z\|^2}{2 \sigma^2} \right)
  $$
  对应的支持向量机是一个高斯径向基函数分类器。

### 7.3.4 非线性支持向量分类机

- 学习算法

  - 选取适当的核函数 $K(x, z)$ 和适当的参数 $C$ ，构造并求解最优化问题
    $$
    \begin{aligned}
    & \min_\alpha \ \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j K(x_i, x_j) - \sum_{i=1}^N \alpha_i \\
    &  \ \text{s.t.} \qquad \sum_{i=1}^N \alpha_i y_i = 0 \\
    &  \qquad \qquad 0 \le \alpha_i \le C , \ \ i =1, 2, \dots, N
    \end{aligned}
    $$
    求得最优解 $\alpha^* = (\alpha_1^*, \alpha_2^*, \dots, \alpha_N^*)^T $ 。

  - 选择 $\alpha^*$ 的一个正分量 $0 < \alpha_j^* < C$ ，计算
    $$
    b^* = y_j - \sum_{i=1}^N \alpha_i^* y_i K(x_j, x_j)
    $$

  - 构造决策函数
    $$
    f(x) = \text{sign}\left(\sum_{i=1}^{N_s}\alpha_i^* y_i K(x_i, x) + b^* \right)
    $$

  - 当 $K(x, z)$ 是正定核时，上述问题是凸二次规划问题，解是存在的。

## 7.4 序列最小最优化算法

- 概述

  - 用于支持向量机学习的高效实现。
  - 解决训练样本容量很大时，许多最优化算法变得低效甚至无法使用的问题。

- 基本思路

  SMO 算法是一种启发式算法，其基本思路是：

  - 如果所有变量的解都满足此最优化问题的 KKT 条件，那么这个最优化问题的解就得到了，因为 KKT 条件是该最优化问题的充分必要条件。
  - 否则，选择两个变量，固定其它变量，针对这两个变量构建一次二次规划问题。
  - 如此，SMO 算法将原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的。

- 注：

  - 子问题关于这两个变量的解应该更接近原始二次规划问题的解，而且子问题可以通过解析方法求解，可以大大提高整个算法的计算速度。

  - 子问题有两个变量，一个是违反 KKT 条件最严重的那一个，另一个由约束条件自动确定。

  - 由于存在等式约束
    $$
    \sum_{i=1}^N \alpha_i y_i = 0
    $$
    子问题的两个变量中只有一个是自由变量。

- SMO 算法包括两个部分：

  - 求解两个变量二次规划的解析方法；
  - 选择变量的启发式方法。

