# 第8章 提升方法

- 概述

  提升方法是一种常用的统计学习方法，应用广泛且有效。在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行组合，提高分类性能。

## 8.1 提升方法 AdaBoost 算法

### 8.1.1 提升方法的基本思路

- 基本思想：

  对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比任何一个专家单独的判断好。

- AdaBoost的思路

  - 改变训练数据的权值或概率分布的方法：

    提高那些被前一轮弱分类器误分类样本的权值，而降低那些被正确分类样本的权值，从而使那些没有得到正确分类的样本由于其权值的加大，受到后一轮弱分类器的更大关注。

  - 弱分类器的组合方法：

    采用加权多数表决的方法，具体地，加大分类误差率小的弱分类器的权值，减小分类误差率大的弱分类器的权值。

### 8.1.2 AdaBoost 算法

- 算法：

  - 初始化训练数据的权值分布
    $$
    D_1 = (w_{11}, w_{12}, \dots, w_{1N}), \ \ w_{1i} = \frac{1}{N},  \ \ i = 1, 2, \dots, N
    $$

  - 对 $m = 1, 2, \dots, M$ 

    - 使用具有权值分布 $D_m$ 的训练数据集学习，得到基本分类器
      $$
      G_m(x) : \mathcal{X} \rightarrow \{-1, +1\}
      $$

    - 计算 $G_m(x)$ 在训练数据集上的分类误差率
      $$
      e_m = \sum_{i=1}^N P(G_m(x_i) \ne y_i) = \sum_{i=1}^N w_{mi} I(G_m(x_i) \ne y_i )
      $$

    - 计算 $G_m(x)$ 的系数
      $$
      \alpha_m = \frac{1}{2} \log \frac{1-e_m}{e_m}
      $$
      这里的对数是自然对数

    - 更新训练数据集的权值分布
      $$
      D_{m+1} = (w_{m+1, 1}, w_{m+1, 2}, \dots, w_{m+1, N}) \\
      w_{m+1, i} = \frac{w_{mi}}{Z_m} \exp(-\alpha_m y_i G_m(x_i)), \ \ i = 1, 2, \dots, N
      $$
      这里，$Z_m$ 是规范化因子
      $$
      Z_m = \sum_{i=1}^N w_{mi} \exp(-\alpha_m y_i G_m(x_i))
      $$

    - 构建基本分类器的线性组合
      $$
      f(x) = \sum_{m=1}^M \alpha_m G_m(x)
      $$
      得到最终分类器
      $$
      G(x) = \text{sign}(f(x)) = \text{sign}\left(\sum_{m=1}^M \alpha_m G_m(x) \right)
      $$


## 8.2 AdaBoost 算法的训练误差分析

- AdaBoost 算法最终分类器的误差界为
  $$
  \frac{1}{N} \sum_{i=1}^N I(G(x_i) \ne y_i) \le \frac{1}{N} \sum_{i=1}^N \exp(-y_i f(x_i)) = \prod_{m=1}^M Z_m
  $$

- 解释

  这一定理说明，可以在每一轮选取适当的 $G_m$ 使得 $Z_m$ 最小，从而使训练误差下降最快。

## 8.3 AdaBoost 算法的解释

- 认为 AdaBoost 算法是模型为加法模型、损失函数为指数函数、学习算法为前向分布算法时的二类分类学习方法。

## 8.4 提升树

### 8.4.1 提升树模型

- 提升树：以决策树为基函数的提升方法。

  - 对于分类问题，决策树是二叉分类树；
  - 对于回归问题，决策树是二叉回归树。

- 提升树模型可以表示为决策树的加法模型：
  $$
  f_M(x) = \sum_{m=1}^M T(x; \Theta_m)
  $$
  其中，$T(x; \Theta_m) $ 表示决策树，$\Theta_m$ 表示决策树的参数，$M$ 为树的个数。

### 8.4.2 提升树算法

- 前向分步算法：

  首先确定初始提升树 $f_0(x) = 0$ ，第 $m$ 步的模型是
  $$
  f_m(x) = f_{m-1}(x) + T(x; \Theta_m)
  $$
  其中，$f_{m-1}(x)$ 为当前模型，通过经验风险极小化确定下一棵决策树的参数 $\Theta_m$ ，
  $$
  \hat \Theta_m = \arg \min_{\Theta_m} \sum_{i=1}^N L(y_i, f_{m-1}(x_i) + T(x_i; \Theta_m))
  $$

- 针对不同问题的提升树学习算法，主要区别在于使用的损失函数不同

  - 回归问题：使用平方损失函数；
  - 分类问题：使用指数损失函数。

### 8.4.3 梯度提升

- 梯度提升用来解决使用一般损失函数的提升树优化困难的问题。

- 梯度提升算法

  - 初始化
    $$
    f_0 (x) = \arg \min_c \sum_{i=1}^N L(y_i, c)
    $$

  - 对 $m=1,2, \dots, M$ 

    - 对 $i = 1,2, \dots, N$ ，计算
      $$
      r_{mi} = - \left[ \frac{\partial L(y_i, f(x_i))}{\partial f(x_i)} \right]_{f(x) = f_{m-1}(x)}
      $$

    - 对 $r_{mi}$ 拟合一个回归树，得到第 $m$ 棵树的叶结点区域 $R_{mj}$ ，$j=1,2,\dots, J$ 

    - 对 $j = 1,2, \dots, J$ ，计算
      $$
      c_{mj} = \arg \min_c \sum_{x \in R_{mj}} L(y_i, f_{m-1}(x_i)+c)
      $$

    - 更新 $f_m(x)$
      $$
      f_m(x) = f_{m-1}(x) + \sum_{j=1}^J c_{mj} I(x \in R_{mj})
      $$

  - 得到回归树
    $$
    \hat f (x) = f_M(x) = \sum_{m=1}^M \sum_{j=1}^J c_{mj} I(x \in R_{mj})
    $$
    ​

